{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome","title":"Welcome","text":"<p>Pyxy3D (pixie-3d) is an open-source Python package for converting 2D (x,y) point data to 3D estimates. It is intended to serve as the calibration and triangulation workhorse of a low-cost DIY motion capture studio. It's core functionality includes: </p> <ul> <li>the estimation of intrinsic (focal length/optical center/distortion) and extrinsic (rotation and translation) camera parameters via a GUI</li> <li>API for slotting various tracking solutions into the data pipeline</li> <li>triangulation of tracked points</li> </ul> <p>The packages comes included with a sample tracker using Google's Mediapipe which illustrates how to use the tracker API. The camera management backend allows for recording of synchronized frames from connected webcams, though the frame rate/resolution/number of cameras will be limited by the bandwidth of the current system.</p> <p>This project is at a very early stage so please bear with us while going through the inevitable growing pains that are ahead. You feedback is appreciated. If you have specific recommendations, please consider creating an issue. If you have more general questions or thoughts about the project, please open up a thread in the discussions.</p> <p>If you are just starting out here and trying to get a basic handle on what this is, what it does, and how it is used, it might be best to dive into the FAQ, or just watch this quick demo of a calibration and capture session:</p> <p>From there, the installation guide will walk you through the process of getting a system up and running, the calibration guide will help you get it dialed in, and the motion capture guide will walk through the process of collecting data.</p> <p>Please note that landmark position data is currently only being exported to <code>csv</code> and <code>trc</code> (used by biomechanists) formats, which very likely is not going to satisfy the needs of animators. If you have experience in character rigging and python programming and are interested in contributing, please reach out in discussions to get the ball rolling on a game plan. Pull requests are welcome!</p>"},{"location":"calibration/","title":"Calibration","text":"<p>This is a placeholder for future documentation that is currently on the todo list. Please see the demo video for how to calibrate your camera system. </p>"},{"location":"data_capture/","title":"Data Capture","text":"<p>This is a placeholder for future documentation that is currently on the todo list. Please see the demo video for a sample data capture. </p>"},{"location":"doc_notes/","title":"Reminders for using MkDocs","text":"<p>This will likely get deleted soon, but I just want a place to store reminders of the workflow here...</p> <p>For full documentation visit mkdocs.org. test</p>"},{"location":"doc_notes/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"doc_notes/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#does-it-do-real-time-processing","title":"Does it do real-time processing?","text":"<p>While the data processing pipeline is designed with the ultimate goal of real-time tracking, the current version does not support it. The processing demands of landmark detection across concurrent frames currently throttles the frame rate to such an extent that I don't consider this a worthile investment of time at the moment. As a stack of hardware/tracking algorithm emerges that shows a viable path to a scaleable system, this will get bumped as a priority. If you have expertise in this area and are interested in contributing, please consider opening up a thread in the discussions to start a conversation.</p>"},{"location":"faq/#can-i-process-videos-i-pre-recorded-with-gopros-etc","title":"Can I process videos I pre-recorded with GoPros, etc?","text":"<p>Not currently, but this feature is planned for near-term roll out. Processing videos offline will enable the use of more cameras and higher frame rates and resolutions, but it also requires some method of frame synchronization and presents the need to perform calibration from pre-recorded videos. This is a development priority, but not currently implemented. I aim to create an API that will support such post-processing in the future so that Pyxy3D could be used programmatically by third-party Python processing pipelines.</p>"},{"location":"faq/#can-i-use-my-smartphone-as-a-camera","title":"Can I use my smartphone as a camera?","text":"<p>Unfortunately, no. Supporting such input streams would present a unique challenge that would detract from the development of core processing. Including this ability is not a priority for development.</p>"},{"location":"faq/#which-webcam-should-i-purchase","title":"Which webcam should I purchase?","text":"<p>I would recommend starting small and cheap and go from there. Start with whatever webcams you currently have access to. Conduct tests with two cameras to get a feel of how things run on your local system before throwing down for more. Scale out from there.</p> <p>I will note that I have had success with the EMeet HD1080p cameras, which are reasonably priced (~$25 on Amazon). More expensive cameras with additional features such as autofocus have presented complications in my experience. If you have had a positive or negative experience with a specific webcam, kindly share it on our discussions page.</p>"},{"location":"faq/#can-the-software-export-to-blender-or-unrealmayaetc","title":"Can the software export to Blender (or Unreal/Maya/etc)?","text":"<p>Currently, the software only exports unfiltered 3D estimates in <code>csv</code> and <code>trc</code> formats. The <code>trc</code> format is designed for biomechanists. Those interested in creating an output pipeline to other formats may find the 'csv' files a good starting point and I invite you to open up a discussion if you would like to talk through code.</p>"},{"location":"faq/#what-is-happening-with-my-data-are-you-storing-videos-i-record","title":"What is happening with my data? Are you storing videos I record?","text":"<p>Absolutely not. All operations are performed locally on your machine. An imagined future use-case for this package is as a tool that could be used in clinical settings or human subjects research. Data privacy is absolutely critical under those circumstances. The commitment that you will always control your data is at the heart of this project. </p>"},{"location":"file_layout/","title":"Post Processing Pipeline","text":"<p>Following recording, the <code>mp4</code> files, augmented by <code>frame_time_history.csv</code> and <code>config.toml</code> (needed for camera rotation count), are used to generate (x,y) coordinates via the <code>Tracker</code>. This is generally the most time consuming portion of the post-processing. A subdirectory will be created that contains the (x,y) file as well as the video file swith frame points displayed.</p> <p>This (x,y) file can then be used along with the <code>config.toml</code> (for the camera intrinsics and extrinsics) to generate <code>xyz_TRACKER.csv</code> within this <code>TRACKER</code> subdirectory.</p> <p>The (x,y,z) points here are only identified via point ID. The <code>Tracker</code> can be used to generate a labelled file in wide format. This labelled version of the file can then be used to generate a <code>trc</code> file which can be used by OpenSim. </p> <p>It's not intended for users to manage this granular file production themselves, but this documentation is intended for development record keeping and to provide context on the project file structure and multitude of files produced.</p> <p>The following shows the layout of a 3 camera project file with 1 recording where it has been processed by a HOLISTIC tracker with output to a trc file for use in OpenSim.</p> <pre><code>Project/\n\u251c\u2500\u2500 calibration\n\u251c\u2500\u2500 config.toml\n\u2514\u2500\u2500 recording_1\n    \u251c\u2500\u2500 frame_time_history.csv\n    \u251c\u2500\u2500 port_0.mp4\n    \u251c\u2500\u2500 port_1.mp4\n    \u251c\u2500\u2500 port_2.mp4\n    \u2514\u2500\u2500 HOLISTIC\n        \u251c\u2500\u2500 frame_time_history.csv\n        \u251c\u2500\u2500 port_0_HOLISTIC.mp4\n        \u251c\u2500\u2500 port_1_HOLISTIC.mp4\n        \u251c\u2500\u2500 port_2_HOLISTIC.mp4\n        \u251c\u2500\u2500 xy_HOLISTIC.csv\n        \u251c\u2500\u2500 xyz_HOLISTIC.csv\n        \u251c\u2500\u2500 xyz_HOLISTIC_labelled.csv\n        \u2514\u2500\u2500 xyz_HOLISTIC.trc\n</code></pre>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#highlights","title":"Highlights","text":"<ul> <li>Pyxy3D is installable via pip and the GUI can be launched from the command line. </li> <li>It is strongly advised that you do so within a virtual environment. </li> <li>The package requires Python 3.10  or higher. </li> <li>Because the Mediapipe implementation only works on Windows currently, these steps assume you are installing on Windows 10.</li> </ul>"},{"location":"installation/#1-create-a-virtual-environment","title":"1. Create a virtual environment","text":"<p>Find the path to your python.exe file. You can install Python 3.10 from here. For me the path is <code>C:\\Python310\\python.exe</code></p> <p>Create a folder where you would like the virtual environment to live. This can be different from the folder where your motion capture calibration and recording data is stored.</p> <p>Launch a terminal (on Windows, search 'powershell' in the start menu and launch that). Run the following at the command prompt, substituting in the path to <code>python.exe</code> that is true for your machine <pre><code>C:\\Python310\\python.exe -m venv .venv\n</code></pre></p> <p>This will create a fresh version of python within that folder which you will use to manage your project. Activate the environment using the following command (if this exact command doesn't work, then some other variation will) <pre><code>.\\.venv\\Scripts\\activate\n</code></pre></p> <p>The terminal should now show the environment is activated with something like this green parenthetical:</p> <p></p> <p>You can confirm that your python path is set by running</p> <p><pre><code>python -c \"import sys; print(sys.executable)\"\n</code></pre> which should point to the file in the virtual environment you created:</p> <p></p>"},{"location":"installation/#2-install-pyxy3d-via-pip","title":"2. Install pyxy3D via pip","text":"<p>You are now ready to install pyxy3D from the Python Package Index (PyPI) via pip:</p> <pre><code>pip install pyxy3d\n</code></pre> <p>Installation may take a moment...</p>"},{"location":"installation/#3-launch-from-the-command-line","title":"3. Launch from the command line","text":"<p>With the package installed and the virtual environment activated, the main GUI can be launched by running the following command to launch the tool:</p> <pre><code>pyxy3d\n</code></pre> <p>If you experience crashes after initializing the session folder, then you can launch the individual interface components one at a time as needed. NAVIGATE TO THE FOLDER OF THE SESSION YOU WANT TO LAUNCH and run one of the following as needed: <code>charuco</code>, <code>cameras</code>, <code>calibrate</code>, <code>record</code>, <code>process</code></p> <p>For example, if you are getting crashes when trying to record, within the terminal navigate to the session folder you previously created and run:</p> <pre><code>pyxy3d record\n</code></pre> <p>A recording widget will open up that should be more efficient and stable than the complete GUI.</p>"},{"location":"project_structure/","title":"Code Map","text":""},{"location":"project_structure/#current-flow","title":"Current Flow","text":"<p>The general flow of processing is illustrated in the graph below. This is not intended to be useful to anyone other than those involved in programming of core processes. If that is not you, then feel free to ignore.</p> <p>Note that this diagram requires updating after recent refactors. </p> <pre><code>graph TD\n\n\nLiveStream --FramePacket--&gt; Synchronizer\nRecordedStream --FramePacket--&gt; Synchronizer\nSynchronizer --SyncPacket--&gt; VideoRecorder\n\nsubgraph cameras\n    Camera --&gt; LiveStream\nend\n\nsubgraph tracking\n    Charuco --&gt; CornerTracker\n    CornerTracker --PointPacket--&gt; LiveStream\nend\n\nsubgraph GUI\n    MonoCalibrator\n    StereoFrameBuilder\nend\n\nSynchronizer --SyncPacket--&gt;  StereoFrameBuilder\n\nLiveStream -.FramePacket.-&gt; MonoCalibrator\nMonoCalibrator -.Intrinsics.-&gt; config.toml\n\nVideoRecorder --&gt; frame_time_history.csv\nVideoRecorder --&gt; port_X.mp4 \nVideoRecorder -.During StereoFrameBuilder.-&gt; xy.csv\nport_X.mp4 --&gt; RecordedStream\nframe_time_history.csv --&gt; RecordedStream\n\n\nsubgraph recording\n\n    RecordedStream\n    VideoRecorder \n\n    subgraph RecordingDirectory\n        port_X.mp4\n        frame_time_history.csv\n    end\n\nend\n\n\nxy.csv --&gt; StereoCalibrator\nconfig.toml --CameraSettings--&gt; StereoCalibrator\nStereoCalibrator -.StereoPairs.-&gt; config.toml\n\n\nCameraArray --&gt; SyncPacketTriangulator\nSynchronizer -.SyncPacket.-&gt; SyncPacketTriangulator\nSyncPacketTriangulator -.XYZPacket.-&gt; TrackedPointVizualizer\nCameraMesh --&gt; TrackedPointVizualizer\n\n\nsubgraph calibration_data\n    xy.csv\n    config.toml\nend\n\nxy.csv --&gt; get_stereotriangulated_table\n\nArrayStereoTriangulator --&gt; get_stereotriangulated_table\n\nCornerTracker --PointPacket--&gt; RecordedStream\n\nCameraArrayInitializer --&gt; CameraArray\nconfig.toml --&gt; CameraArrayInitializer\n\nCameraArray --&gt; ArrayStereoTriangulator\n\n\nsubgraph triangulate\n    ArrayStereoTriangulator\n    StereoPointsBuilder --- ArrayStereoTriangulator\n    StereoPairTriangulator --- ArrayStereoTriangulator\nend\n\nCaptureVolume -.via Session.-&gt; CaptureVolumeVisualizer\n\nget_point_estimates  --&gt; PointEstimates\n\n\nsubgraph capture_volume\n    subgraph helper_functions\n        get_stereotriangulated_table -.stereotriangulated_table DF.-&gt; get_point_estimates   \n    end\n\n    CameraArray --&gt; CaptureVolume\n    PointEstimates --&gt; CaptureVolume\n    CaptureVolume --&gt; QualityScanner\n    QualityScanner -.filtered.-&gt; PointEstimates\nend\n\n\nsubgraph visualization\n    CaptureVolumeVisualizer\n    CameraMesh --&gt; CaptureVolumeVisualizer\nend\n</code></pre>"},{"location":"requirements/","title":"Minimum Requirements","text":"<ul> <li> <p>Python version: Python 3.10 or later.</p> </li> <li> <p>Hardware: At least two webcams are needed, though tracking accuracy improves with the use of more webcams. A reasonably mid-tier PC is necessary. Low-powered machines, such as laptops or low-end desktop PCs, may struggle with frame reading and image processing tasks.</p> </li> <li> <p>Operating System: While the core calibration and GUI are cross-platform, Google's MediaPipe, implemented only on CPU and only for Windows, is used for the sample tracking algorithm. It has only been tested on Windows 10.</p> </li> <li> <p>Calibration Board: A ChArUco board is needed for camera calibration and determining spatial relationships between multiple cameras. A sample board can be printed from the GUI on a standard 8.5 x 11 sheet of paper. It is crucial to place the printed board on a flat surface to ensure accurate calibration, such as taping it down to a rigid flat piece of cardboard.</p> </li> <li> <p>Capture Environment: Data recording requires a well-lit and evenly lit environment. It is beneficial if the background contrasts highly with the individual being tracked. For example, tracking difficulties may arise if a person in dark clothing stands against a similarly dark wall. Be mindful of clothing, background, and lighting to optimize the quality of the captured data.</p> </li> </ul>"}]}